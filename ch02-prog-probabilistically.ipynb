{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e9157f-ffc9-44f5-b020-7422e86c0dd3",
   "metadata": {},
   "source": [
    "## Ch. 02- Programming Probabilistically"
   ]
  },
  {
   "cell_type": "code",
   "id": "79282b3286f3013b",
   "metadata": {},
   "source": [
    "# Import pymc and related code\n",
    "import arviz as az\n",
    "import pymc as pm\n",
    "import preliz as pz"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "# Import other \"data science libraries\"\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c5b137c5-a8cc-426e-ac45-22a6225b2c8d",
   "metadata": {},
   "source": [
    "## 2.1 Probabilistic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d175cd-8621-4016-9308-1dd24ed40933",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T03:12:01.187867Z",
     "start_time": "2025-01-14T03:12:01.185864Z"
    }
   },
   "source": [
    "#### 2.1.1 Flipping coins the PyMC way2.1.1 Flipping coins the PyMC way"
   ]
  },
  {
   "cell_type": "code",
   "id": "62c6b0dc-3f6b-42e3-ba20-ad6eb1f791aa",
   "metadata": {},
   "source": [
    "# Initialize repeatable random number generator\n",
    "rng = np.random.default_rng(123)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "52045298c1db82ce",
   "metadata": {},
   "source": [
    "# Generate \"fake real data\"\n",
    "trials = 4\n",
    "theta_real = 0.35 # unknown in a real experiment\n",
    "data = pz.Binomial(\n",
    "    n=1,\n",
    "    p=theta_real).rvs(trials,\n",
    "                      random_state=rng.integers(np.iinfo(np.int32).max))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b189ba0c0116146d",
   "metadata": {},
   "source": [
    "plt.scatter(range(trials), data)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "78d7fed8-92b6-4228-aedd-4262441a2e20",
   "metadata": {},
   "source": [
    "with pm.Model() as our_first_model:\n",
    "    θ = pm.Beta('θ', alpha=1., beta=1.)\n",
    "    y = pm.Bernoulli('y', p=θ, observed=data)\n",
    "    idata = pm.sample(1000)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0d5c8008-bedd-48c0-ace6-ece7071d373b",
   "metadata": {},
   "source": [
    "### 2.2 Summarizing the posterior"
   ]
  },
  {
   "cell_type": "code",
   "id": "b1aa1f02-dcb4-4067-8312-d4b4e444d133",
   "metadata": {},
   "source": [
    "az.plot_trace(idata)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8e91b3e3-1f33-4b23-95ee-6d46dfe5a358",
   "metadata": {},
   "source": [
    "az.plot_trace(idata, kind='rank_bars', combined=True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e39e0a99-e9e8-42cc-aef5-ebfe4328766f",
   "metadata": {},
   "source": [
    "az.plot_posterior(idata)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8d4ea240-9b82-4f94-97e4-a8e3905012d7",
   "metadata": {},
   "source": [
    "### 2.3 Posterior-based decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb44415e-b7b0-4541-8fa4-9d11e9c283d6",
   "metadata": {},
   "source": [
    "#### 2.3.1 Savage-Dickey density ratio"
   ]
  },
  {
   "cell_type": "code",
   "id": "461dcfc1-acd9-4f9d-925d-92ea4ce8ffff",
   "metadata": {},
   "source": [
    "az.plot_bf(idata, var_name='θ', prior=rng.uniform(0, 1, 10000), ref_val=0.5)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "daec4ead-0ef8-4736-b35e-7c312985a77e",
   "metadata": {},
   "source": [
    "#### 2.3.2 Region of Practical Equivalence"
   ]
  },
  {
   "cell_type": "code",
   "id": "ad6c9a20-1001-42b8-9e19-466b59e2c7a1",
   "metadata": {},
   "source": [
    "az.plot_posterior(idata, rope=[0.45, 0.55])\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "93d74b9e-142f-4024-adc0-cdf35539ada7",
   "metadata": {},
   "source": [
    "az.plot_posterior(idata, ref_val=0.5)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a687f859-eba3-4f39-8520-15b9dff02a0d",
   "metadata": {},
   "source": [
    "#### 2.3.3 Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "4efc5923-3e96-48cf-89f1-7432f70c3775",
   "metadata": {},
   "source": [
    "# Plot the loss\n",
    "# The plotting part of this code is from \n",
    "# [the chapter 02 code](https://github.com/aloctavodia/BAP3/blob/main/code/Chp_02.ipynb).\n",
    "grid = np.linspace(0, 1, 200)\n",
    "θ_pos = idata.posterior['θ']\n",
    "lossf_a = [np.mean(abs(i - θ_pos)) for i in grid]\n",
    "lossf_b = [np.mean((i - θ_pos) ** 2) for i in grid]\n",
    "\n",
    "_, ax = plt.subplots(figsize=(12, 3))\n",
    "for lossf, c in zip([lossf_a, lossf_b], ['C0', 'C1']):\n",
    "    mini = np.argmin(lossf)\n",
    "    ax.plot(grid, lossf, c)\n",
    "    ax.plot(grid[mini], lossf[mini], 'o', color=c)\n",
    "    ax.annotate('{:.2f}'.format(grid[mini]),\n",
    "                (grid[mini], lossf[mini] + 0.03),\n",
    "                color=c)\n",
    "\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(r'$\\hat \\theta$')\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ee5ce6c0-0963-4664-bffb-5c1ac32ebef8",
   "metadata": {},
   "source": [
    "# A (silly) assymetric loss function\n",
    "lossf = []\n",
    "for i in grid:\n",
    "    if i < 0.5:\n",
    "        f = 1 / np.median(θ_pos / np.abs(i**2 - θ_pos))\n",
    "    else:\n",
    "        f = np.mean((i - θ_pos) ** 2 + np.exp(-i)) - 0.25\n",
    "\n",
    "    lossf.append(f)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9bec79d2-7a0b-427f-87cd-9246c2222d68",
   "metadata": {},
   "source": [
    "# Plot the (silly) asymmetric loss function\n",
    "mini = np.argmin(lossf)\n",
    "_, ax = plt.subplots(figsize=(12, 3))\n",
    "ax.plot(grid, lossf)\n",
    "ax.plot(grid[mini], lossf[mini], 'o')\n",
    "ax.annotate('{:.2f}'.format(grid[mini]),\n",
    "(grid[mini] + 0.01, lossf[mini] + 0.1))\n",
    "ax.set_yticks([])\n",
    "ax.set_xlabel(r'$\\hat \\theta$')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "116a0ae298773bca",
   "metadata": {},
   "source": [
    "### 2.4 Gaussians all the way down"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5544a09b7396ae2",
   "metadata": {},
   "source": [
    "Gaussians are very appealing. They are easy to work with,\n",
    "many operations applied to Gaussians return another Gaussian.\n",
    "Additionally, many natural phenomena can be approximated using\n",
    "Gaussians. In general, almost every time we measure the average\n",
    "of something, using a **big enough** sample size, the average\n",
    "will be distributed as a Gaussian.\n",
    "\n",
    "Many phenomena are indeed averages. For example, the height\n",
    "of adults. (Actually, this distribution is a **mixture** of\n",
    "**two** Gaussians - one for men and one for women.)\n",
    "\n",
    "Consequently, it is important to learn to build Gaussians,\n",
    "but also to learn how to relax the normality assumptions.\n",
    "(This relaxation is surprisingly easy with tools like PyMC).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261326b21e360a2f",
   "metadata": {},
   "source": [
    "#### 2.4.1 Gaussian inferences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633ce18230e3a5b5",
   "metadata": {},
   "source": [
    "**Background**\n",
    "\n",
    "We can use nuclear magnetic Resonance (NMR) to study molecules or\n",
    "living things such as humans, sunflowers, and yeast. NMR allows one\n",
    "to measure different **observable** quantities related to **unobservable**\n",
    "molecular properties. Chemical shift is one of these observable\n",
    "properties that apply to the nuclei of certain types of atoms.\n",
    "This problem is an example similar to:\n",
    "\n",
    "- The height of a group of people\n",
    "- The average time to travel back home\n",
    "- The weights of bags or oranges\n",
    "\n",
    "All these examples have continuous variables and can be thought of as an\n",
    "average plus a dispersion.\n",
    "\n",
    "Additionally, if the number of possible values is large enough, we can\n",
    "approximate it using a Gaussian. For example, the sexual partners of\n",
    "bonobos, a very promiscuous monkey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838833d6a6b8a1d5",
   "metadata": {},
   "source": [
    "In our example, we have 48 chemical shift value.\n",
    "\n",
    "- The median is around 53\n",
    "- The inter-quartile range is about 52 to 55\n",
    "- Two values \"far away\" from the resto of the data appear to be outliers."
   ]
  },
  {
   "cell_type": "code",
   "id": "c98819296c763cac",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "data = np.loadtxt('./data/chemical_shifts.csv')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e7bd07be5aef1ace",
   "metadata": {},
   "source": [
    "# Plot the data using a boxplot\n",
    "_, ax = plt.subplots(figsize=(12, 3))\n",
    "ax.boxplot(data, vert=False)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7d1f7af0c85e448f",
   "metadata": {},
   "source": [
    "We'll forget about the two outlying points. We will further assume that\n",
    "a Gaussian is a good description of the data. Since know neither the mean\n",
    "nor the standard deviation, we set priors for both of them. Therefore, a\n",
    "reasonable model is:\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\mu \\sim \\mathcal{U(l, h)} \\\\\n",
    "\\sigma \\sim \\mathcal{HN(\\sigma_{\\sigma})} \\\\\n",
    "Y \\sim \\mathcal{N(\\mu, \\sigma)}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\mathcal{U(l, h)}$ is the Uniform distribution between\n",
    "  $\\mathcal{l}$ and $\\mathcal{h}$\n",
    "- $\\mathcal{HN(\\sigma_{\\sigma})}$ is the Half-Normal distribution\n",
    "  with scale $\\mathcal{\\sigma_{\\sigma}}$\n",
    "- $\\mathcal{N(\\mu, \\sigma)}$ is the Gaussian distribution with mean,\n",
    "  $\\mathcal{\\mu}$, and standard deviation, $\\mathcal{\\sigma}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a653d5744030c3ba",
   "metadata": {},
   "source": [
    "Since we do not know the possible values of $\\mu$ and $\\sigma$ - a typical\n",
    "situation - we can set priors reflecting our ignorance. For example, we\n",
    "can set the boundaries of our uniform distribution to be\n",
    "$\\mathcal{l} = 40$ and $\\mathcal{h} = 75$: a range **larger** than the\n",
    "range of the data.\n",
    "\n",
    "For the Half-Normal, in the absence of more information, we can choose a\n",
    "large value compared to the **scale** of the data. The following PyMC\n",
    "code puts details to our model."
   ]
  },
  {
   "cell_type": "code",
   "id": "d943d0cc3b25231c",
   "metadata": {},
   "source": [
    "with pm.Model() as model_g:\n",
    "    mu = pm.Uniform('\\u03bc', lower=40, upper=70)\n",
    "    sigma = pm.HalfNormal('\\u03c3', sigma=5)\n",
    "    Y = pm.Normal('Y', mu=mu, sigma=sigma, observed=data)\n",
    "    idata_g = pm.sample()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6cdc32828cc9e007",
   "metadata": {},
   "source": [
    "az.plot_trace(idata_g)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "652e94c5b0935d92",
   "metadata": {},
   "source": [
    "az.plot_pair(idata_g, kind='kde', marginals=True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a1d66b590651b748",
   "metadata": {},
   "source": [
    "az.summary(idata_g, kind='stats').round(2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5a0cc88f826fe32",
   "metadata": {},
   "source": [
    "### Posterior predictive checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e610810d1f8d8a",
   "metadata": {},
   "source": [
    "One nice element of the Bayesian toolkit. Once one has calculated\n",
    "the posterior, $P(\\theta | Y)$, one can use that posterior to\n",
    "generate predictions, $p(\\tilde{Y})$.\n",
    "\n",
    "Although calculating the posterior involves an integral, using PyMC\n",
    "to get posterior predictive samples involves:\n",
    "\n",
    "- Calling the function, `sample_posterior_predictive()`\n",
    "- Passing `InferenceData` as the first object\n",
    "\n",
    "Additionally, we must pass the `model` object, and we **may** use\n",
    "the `extend_Inferencedata` argument to add the posterior predictive\n",
    "samples to the `InferenceData` object."
   ]
  },
  {
   "cell_type": "code",
   "id": "b6cbf482ed5200d0",
   "metadata": {},
   "source": [
    "pm.sample_posterior_predictive(\n",
    "    idata_g,\n",
    "    model=model_g,\n",
    "    extend_inferencedata=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dc97619a5c890ab2",
   "metadata": {},
   "source": [
    "Additionally, we can plot the posterior predictive checks."
   ]
  },
  {
   "cell_type": "code",
   "id": "d9c4e9e9bc11ac2e",
   "metadata": {},
   "source": [
    "az.plot_ppc(idata_g, num_pp_samples=100)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3ced7332483b37ed",
   "metadata": {},
   "source": [
    "This plot contains\n",
    "\n",
    "| Curve | Explanation |\n",
    "| ----- | ----------- |\n",
    "| White | Kernel Density Estimate (KDE) of the observed data |\n",
    "| Blue | KDEs computed from each of the 100 posterior predictive samples |\n",
    "| Orange | Posterior predictive mean |\n",
    "\n",
    "The blue lines reflect our uncertainty of the predicted data. The plots are\n",
    "_hairy_ or _wonky_ to reflect our (relative) **lack of data**.\n",
    "We can make additional observations about our simulated data. For example,\n",
    "the mean of the simulated data is displaced to the right with a slightly\n",
    "larger variance that the variance of the actual data.\n",
    "\n",
    "This discrepancy results from\n",
    "\n",
    "- Our choice of likelihood\n",
    "- The two observations away from the bulk of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34347e990028356",
   "metadata": {},
   "source": [
    "How do we interpret this plot? Is the model wrong or right? Can we use it\n",
    "or do we need a different model?\n",
    "\n",
    "Well, it depends. The interpretation of a model and its evaluation and\n",
    "criticism are **always** context dependent.\n",
    "\n",
    "The author, based on his experience, believes that this model is a\n",
    "reasonable enough representation of the data **and** a useful one\n",
    "for most analyses.\n",
    "\n",
    "However, we could find other models the better fit the whole data set,\n",
    "including the two observations that are **far** from the bulk of the\n",
    "data. We'll see how we can do that next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9710f5573fa583ba",
   "metadata": {},
   "source": [
    "### 2.6 Robust inferences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3e0fa37bf19715",
   "metadata": {},
   "source": [
    "One objection we may have with `model_g` is that we are assuming\n",
    "a Normal distribution. However, as we have mentioned, our data has two\n",
    "points away from the bulk of the data. Our choice of a Normal\n",
    "distribution for the likelihood indirectly assumes that we **do not expect**\n",
    "to see a lot of data points far away from the bulk of the data.\n",
    "\n",
    "Since the tails of the Normal distribution fall off quickly as we move\n",
    "away from the mean, the Normal distribution **is surprised** at the two\n",
    "\"extreme\" points.\n",
    "\n",
    "The distribution \"reacts\" in two ways:\n",
    "\n",
    "- It moves its mean towards those two extreme points\n",
    "- It increases its standard deviation\n",
    "\n",
    "What can we do? We have at least two options:\n",
    "\n",
    "We can check for errors in the data; for example,\n",
    "\n",
    "- During cleaning or preprocessing of the data\n",
    "- Resulting from the malfunction of the measuring equipment\n",
    "\n",
    "This conclusion may not be helpful. For example, if the data was actually\n",
    "collected by someone else, we may not be able to reliable draw\n",
    "these conclusions.\n",
    "\n",
    "Another option is to declare the two points outliers and remove theme from\n",
    "the data. Two common rules for identifying outliers are:\n",
    "\n",
    "- Any point that falls **below** 1.5 x the IQR (inter-quartile range)\n",
    "  from the lower quartile or falls **above** 1.5 times  the IQR from\n",
    "  the upper quartile is considered an outlier.\n",
    "- Any data point that falls below or above N times the standard deviation\n",
    "  of the data is considered an outlier. In this situation, N usually has\n",
    "  a value of 2 or 3.\n",
    "\n",
    "However, as with **any automatic \"method\"**, these rules of thumb\n",
    "**are not perfect** and may result in discarding **valid data points.**\n",
    "\n",
    "As a general rule, Baysians prefer to encode assumptions directly into the\n",
    "model by using different priors and likelihood rather than appealing to\n",
    "_ad hoc_ heuristics such as outlier removal rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a27d15f04cad6",
   "metadata": {},
   "source": [
    "#### 2.6.1 Degree of normality\n",
    "\n",
    "One distribution is very similar to a Normal distribution. It has three\n",
    "parameters and is called the Student's t-distribution:\n",
    "\n",
    "- A location parameter, $\\mu$.\n",
    "- A scale parameter, $\\sigma$.\n",
    "- A normality parameter, $\\nu$. (AKA, the degrees of freedom.)\n",
    "\n",
    "| When...          | Then, the distribution is the... |\n",
    "|------------------|----------------------------------|\n",
    "| $\\nu$ = $\\infty$ | Normal distribution |\n",
    "| $\\nu$ = 1        | Cauchy or Lorentz distribution |\n",
    "\n",
    "The parameter, $\\nu$, has the range [0, $\\infty$]. The lower the\n",
    "value of $\\nu$, the heavier the tails of the distribution.\n",
    "Alternatively, the lower the value of $\\nu$, the higher the\n",
    "kurtosis. Additionally, if $\\nu \\leq 1$, the mean value of\n",
    "the Student's T distribution is not defined. However, remember\n",
    "that we can **always** calculate an empirical mean.\n",
    "\n",
    "Similarly, the variance of the Student's T distribution is only defined\n",
    "for values of $\\nu > 2$. Additionally, the **scale** of the Student's T\n",
    "distribution is **not** the same as its **standard deviation**. However,\n",
    "the scale and the standard deviation become closer and closer as $\\nu$\n",
    "approaches infinity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646a766c56cfa92b",
   "metadata": {},
   "source": [
    "#### 2.6.2 A robust version of the Normal model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562827bb359ca8da",
   "metadata": {},
   "source": [
    "We will rewrite the previous model (`model_g`) by replacing the\n",
    "Gaussian distribution with the Student's T distribution. Because the\n",
    "Student's T distribution has one more parameter, $\\nu$, than the\n",
    "Gaussian, we need to specify one more prior.\n",
    "\n",
    "For this particular problem, we chose to use the exponential distribution,\n",
    "but other distributions restricted to the positive interval could\n",
    "also work.\n",
    "\n",
    "Here is a symbolic representation of our model:\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\mu \\sim \\mathcal{U(l, h)} \\\\\n",
    "\\sigma \\sim \\mathcal{HN(\\sigma_\\sigma)} \\\\\n",
    "\\nu \\sim Exp(\\lambda) \\\\\n",
    "Y \\sim \\mathcal{T(\\nu, \\mu, \\sigma)}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Let's write this model in PyMC. The only cautionary word here is that,\n",
    "by default, the Exponential distribution in PyMC is parameterized with\n",
    "the inverse of the mean.\n",
    "\n",
    "Note that we will set $\\nu$ as an Exponential distribution with a mean\n",
    "of 30, the distribution looks very similar to a Gaussian. Further, we\n",
    "can see that **most of the action** happens for relatively small values\n",
    "of $\\nu$. Consequently, we can say that an Exponential prior with a\n",
    "mean of 30 is a **weakly informative prior**. We generally expect the\n",
    "mean to be around 30, but it can easily move to smaller or larger values.\n",
    "\n",
    "Finally, in many problems, estimating $\\nu$ is of **no direct interest**."
   ]
  },
  {
   "cell_type": "code",
   "id": "50fb3d276df447fa",
   "metadata": {},
   "source": [
    "# Here's our model\n",
    "with pm.Model() as model_t:\n",
    "    mu = pm.Uniform('\\u03bc', lower=40, upper=75)\n",
    "    sigma = pm.HalfNormal('\\u03c3', sigma=10)\n",
    "    nu = pm.Exponential('\\u03bd', 1/30)\n",
    "    y = pm.StudentT('y', nu=nu, mu=mu, sigma=sigma, observed=data)\n",
    "    idata_t = pm.sample()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6ce72cc5fb2aa1ba",
   "metadata": {},
   "source": [
    "# Plot the trace for `model_t`\n",
    "az.plot_trace(idata_t)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5fc01525177cac91",
   "metadata": {},
   "source": [
    "# Print the summary of `model_t`\n",
    "az.summary(idata_t, kind='stats').round(2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "67bde96bb893ed89",
   "metadata": {},
   "source": [
    "# Compare these results to those from `model_g`\n",
    "az.summary(idata_g, kind='stats').round(2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b31a555696192c8b",
   "metadata": {},
   "source": [
    "# Plot the posterior predictive check\n",
    "pm.sample_posterior_predictive(\n",
    "    idata_t,\n",
    "    model=model_t,\n",
    "    extend_inferencedata=True\n",
    ")\n",
    "ax = az.plot_ppc(idata_t,\n",
    "            figsize=(12, 4),\n",
    "            num_pp_samples=100,\n",
    "            mean=False,\n",
    "            colors=['C1', 'C0', 'C1']\n",
    ")\n",
    "ax.set_xlim(40, 70)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1bd57be42badbd5c",
   "metadata": {},
   "source": [
    "### 2.7 InferenceData\n",
    "\n",
    "Contains four groups:\n",
    "\n",
    "- `posterior`\n",
    "- `posterior_predictive`\n",
    "- `sample_stats`\n",
    "- `observed_data`"
   ]
  },
  {
   "cell_type": "code",
   "id": "14b9e096707dca3b",
   "metadata": {},
   "source": [
    "model_g"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f031f0731142565c",
   "metadata": {},
   "source": [
    "idata_g"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4eaf590c574f6bbc",
   "metadata": {},
   "source": [
    "# For example, `InferenceData` allows one to access the posterior data\n",
    "posterior = idata_g.posterior\n",
    "posterior"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d97cd5bc4c3d697a",
   "metadata": {},
   "source": [
    "Many members of `idata_g` return an instance of `xarray`. An `xarray`\n",
    "instance is similar to a NumPy multidimensional array **with labels**.\n",
    "This choice allows a user to \"ignore\" the order of dimensions in\n",
    "the array. For example, the following code returns the first draw\n",
    "from chain 0 and from chain 2."
   ]
  },
  {
   "cell_type": "code",
   "id": "18e0bb121e45f294",
   "metadata": {},
   "source": [
    "posterior.sel(draw=0, chain=[0, 2])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6e69b760b1093589",
   "metadata": {},
   "source": [
    "We use the `sel` method to select a range of values, like the first\n",
    "100 draws from all chains."
   ]
  },
  {
   "cell_type": "code",
   "id": "9704dd65951e56d6",
   "metadata": {},
   "source": [
    "posterior.sel(draw=slice(0, 100))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d00bcf8a63344ad",
   "metadata": {},
   "source": [
    "Additionally, one can return the mean for the data variables,\n",
    "$\\mu$ and $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "id": "e48ffa3e59a828d8",
   "metadata": {},
   "source": [
    "posterior.mean()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9a0e2f0d16c87010",
   "metadata": {},
   "source": [
    "The following code returns the mean **over the draws**; that is,\n",
    "four values for each of $\\mu$ and $\\sigma$, one per chain."
   ]
  },
  {
   "cell_type": "code",
   "id": "52419dea7d2509bb",
   "metadata": {},
   "source": [
    "posterior.mean('draw')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a54541cb64027a48",
   "metadata": {},
   "source": [
    "Most of the time, we do not care about chains and draws; we just want\n",
    "the posterior samples. We can get this information using `az.extract`."
   ]
  },
  {
   "cell_type": "code",
   "id": "9eca76088c4dda62",
   "metadata": {},
   "source": [
    "stacked = az.extract(idata_g)\n",
    "stacked"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8a66c49ff9159710",
   "metadata": {},
   "source": [
    "The `az.extract()` method combines the `chain` and `draw` into a\n",
    "`sample` coordinate upon which we can perform further operations.\n",
    "\n",
    "By default, `extract()` operates on the posterior; however, by\n",
    "specifying other groups with the `group` argument, one can extract data\n",
    "from other groups.\n",
    "\n",
    "One can also use `az.extract()` to get a random sample from the posterior."
   ]
  },
  {
   "cell_type": "code",
   "id": "efcbabbf55cba076",
   "metadata": {},
   "source": [
    "az.extract(idata_g, num_samples=100)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9bb0a9b6528e4682",
   "metadata": {},
   "source": [
    "We will use the `InferenceData` object throughout this book. These many\n",
    "usages will help one to get familiar with it and learn more about it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f47dc97c76e23b6",
   "metadata": {},
   "source": [
    "### 2.8 Groups comparison\n",
    "\n",
    "A common analysis in statistics is group comparison. For example,\n",
    "\n",
    "- How well patients respond to a certain drug\n",
    "- Reduction of car accidents by the introduction of new\n",
    "  traffic regulations\n",
    "- Student performance under different teaching approaches\n",
    "\n",
    "Sometimes, this analysis is framed as hypothesis testing with a goal\n",
    "of declaring a result to be **statistically significant**. This reliance\n",
    "on statistical significance can be problematic. Remember,\n",
    "\n",
    "> Statistical significance is not equivalent to practical significance\n",
    "\n",
    "Hypothesis testing is \"culturally\" connected to the concept\n",
    "of **p-values**. However, interpreting p-values is actually much\n",
    "more difficult than is typically thought.\n",
    "\n",
    "Instead of doing hypothesis testing, we will take a different route and\n",
    "focus on **estimating the effect size**; that is, we want to **quantify\n",
    "the difference between two groups**. Practically, we will move away from\n",
    "yes-no questions like \"Does it work?\" or \"Is there any effect?\" Instead,\n",
    "we will ask more nuanced questions like, \"How well does it work?\" or\n",
    "\"How large is the effect?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2de65cacdcfe7a9",
   "metadata": {},
   "source": [
    "#### 2.8.1 The tips dataset\n",
    "\n",
    "To explore the subject matter of this section, we will use the \"well known\"\n",
    "tips dataset. For this example, different groups are the days of the week.\n",
    "Notice that **no control group exists.** We can arbitrarily establish a\n",
    "control group, for example, Thursday, as the reference or control.\n",
    "\n",
    "We'll start the analysis by loading the dataset as a pandas `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "id": "39a1bfda5ffc326e",
   "metadata": {},
   "source": [
    "tips = pd.read_csv('../data/tips.csv')\n",
    "tips"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f004508b7987541e",
   "metadata": {},
   "source": [
    "Although this `DataFrame` contains many different columns of\n",
    "information, we are only going to use two columns: day and tip."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7229e30934f7dacf",
   "metadata": {},
   "source": [
    "Here is the distribution of this data using a ridge plot from `arviz`."
   ]
  },
  {
   "cell_type": "code",
   "id": "5d073d9d5fcb2d44",
   "metadata": {},
   "source": [
    "az.plot_forest(\n",
    "    tips.pivot(columns='day', values='tip').to_dict('list'),\n",
    "    kind='ridgeplot',\n",
    "    hdi_prob=1,\n",
    "    colors='C1',\n",
    "    figsize=(12, 4),\n",
    ")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c9cf5f5c055858bc",
   "metadata": {},
   "source": [
    "We will perform a small amount of preprocessing of the data."
   ]
  },
  {
   "cell_type": "code",
   "id": "b9bd48268a9a3009",
   "metadata": {},
   "source": [
    "categories = np.array(['Thur', 'Fri', 'Sat', 'Sun'])\n",
    "tip = tips['tip'].values\n",
    "idx = pd.Categorical(tips['day'], categories=categories).codes"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "10c914249aa0ba4f",
   "metadata": {},
   "source": [
    "The model for this problem is almost the same as `model_g`; the only\n",
    "difference is that now, the values $\\mu$ and $\\sigma$ are **vectors**\n",
    "not scalars. PyMC syntax is extremely helpful for this situation.\n",
    "Instead of writing `for` loops, we can write a vectorized model in a\n",
    "straightforward notation."
   ]
  },
  {
   "cell_type": "code",
   "id": "f651f46a1b6d7f5b",
   "metadata": {},
   "source": [
    "with pm.Model() as comparing_groups:\n",
    "    mu = pm.Normal('\\u03bc', mu=0, sigma=10, shape=4) # Four days of the week\n",
    "    sigma = pm.HalfNormal('\\u03c3', sigma=10, shape=4)\n",
    "    y = pm.Normal('y', mu=mu[idx], sigma=sigma[idx], observed=tip)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ab351435c6a46bb8",
   "metadata": {},
   "source": [
    "Notice how we passed a `shape` argument when defining both\n",
    "`mu` and `sigma`. For `mu`, this means we are defining **four\n",
    "independent normal distributions** (that is, four different\n",
    "$\\mathcal{N(0, 10)}$ distributions). Similarly, for $\\sigma$,\n",
    "we are defining four independent half-normal distributions,\n",
    "$\\mathcal{HN(0, 10)}$. Additionally, notice our use of `idx`\n",
    "to index the values of $\\mu$ and $\\sigma$ that we pass to\n",
    "the likelihood.\n",
    "\n",
    "PyMC provides an **alternative syntax** which specifies the\n",
    "\n",
    "- Coordinates\n",
    "- Dimensions\n",
    "\n",
    "The advantage of this alternative is that it allows better integration\n",
    "with ArviZ.\n",
    "\n",
    "In this example, we have four values for the means and four values for\n",
    "the standard deviations. We specify this constraint programmatically by\n",
    "the parameter, `shape=4`. As a result, the generated `InferenceData`\n",
    "object will also be indexed with index values 0, 1, 2, and 3. These\n",
    "index values map to each of our four days ('Thu', 'Fri', 'Sat', 'Sun').\n",
    "\n",
    "In general, we would expect both the programmer and the user to associate\n",
    "these integer values with the common days of the week. By using\n",
    "coordinates and dimensions, both we and ArviZ con use the common labels\n",
    "\n",
    "- Thu\n",
    "- Fri\n",
    "- Sat\n",
    "- Sun\n",
    "\n",
    "We specify two coordinates:\n",
    "\n",
    "- `days` with the dimensions ['Thu', 'Fri', 'Sat', 'Sun']\n",
    "- `days_flat` containing the same labels but repeated according to the\n",
    "  order and length that corresponds to each observation.\n",
    "\n",
    "The coordinate, `days_flat`, will be useful later for\n",
    "posterior predictive tests."
   ]
  },
  {
   "cell_type": "code",
   "id": "631d99348a90c51c",
   "metadata": {},
   "source": [
    "coords = {'days': categories, 'days_flat': categories[idx]}\n",
    "\n",
    "with pm.Model(coords=coords) as comparing_groups:\n",
    "    mu = pm.HalfNormal('\\u03bc', sigma=5, dims='days')\n",
    "    sigma = pm.HalfNormal('\\u03c3', sigma=1, dims='days')\n",
    "    y = pm.Gamma('y', mu=mu[idx], sigma=sigma[idx],\n",
    "                 observed=tip, dims='days_flat')\n",
    "\n",
    "    idata_cg = pm.sample()\n",
    "    idata_cg.extend(pm.sample_posterior_predictive(idata_cg))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "64c6e171de9f86ae",
   "metadata": {},
   "source": [
    "Once the posterior distribution is computed, we can perform all the\n",
    "analyses that we believe are pertinent. For instance, we can perform\n",
    "a posterior predictive test. With the help of ArviZ, wo can perform\n",
    "this test by calling `az.plot_ppc`. We use the `coords` and `flatten`\n",
    "parameters to get one subplot per day."
   ]
  },
  {
   "cell_type": "code",
   "id": "e21d8854c2a19d23",
   "metadata": {},
   "source": [
    "_, axes = plt.subplots(2, 2)\n",
    "az.plot_ppc(idata_cg, num_pp_samples=100,\n",
    "            coords={'days_flat': [categories]},\n",
    "            flatten=[],\n",
    "            ax=axes)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "206600a95128d5c2",
   "metadata": {},
   "source": [
    "From the previous figure, we see that the model captures the general\n",
    "shape of the distribution; however, some details are elusive. This lack\n",
    "may be due to the relatively small sample size, factors other than day\n",
    "influencing the tip size, or a combination of both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5882721a0b55687",
   "metadata": {},
   "source": [
    "For now, we consider that the model is \"good enough\" and move to\n",
    "explore the posterior. We can explain the results in terms of their\n",
    "average values and then find for which days that average is higher.\n",
    "\n",
    "But there are alternatives. For instance, we may want to express the\n",
    "results in terms of differences in posterior means. Additionally, we\n",
    "may want to use some measure of the effect size that is popular with\n",
    "our audience, such as the probability of superiority or Cohen's d.\n",
    "In the next section, we explain these alternatves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5432459cefcda3a",
   "metadata": {},
   "source": [
    "#### 2.8.2 Cohen's d\n",
    "\n",
    "Cohen's d is a common way to measure effect size. It is defined by\n",
    "\n",
    "$$\n",
    "\\frac {\\mu_{2} - \\mu_{1}} {\\sqrt \\frac {\\sigma_{1}^2 + \\sigma_{2}^2} {2}}\n",
    "$$\n",
    "\n",
    "Because we have a posterior **distribution**, we can compute a\n",
    "**distribution** of Cohen's d. We can compute a single value\n",
    "for Cohen's d by computing the mean or the median of the distribution.\n",
    "\n",
    "A Cohen's d can be interpreted as a Z-score (a standard score).\n",
    "A Z-score is the signed number of standard deviations by which a value\n",
    "differs from the mean value of the quantity observed or measured.\n",
    "Consequently, a Cohen's d value of 0.5 can be interpreted as a difference\n",
    "of 0.5 standard deviations from one group to another.\n",
    "\n",
    "A very nice web page to explore what different values of Cohen's d\n",
    "look like is https://rpsychologist.com/d3/cohend. In addition, this page\n",
    "describes other ways to express an effect size, such as the probability\n",
    "of superiority.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dad683840fc1c45",
   "metadata": {},
   "source": [
    "#### 2.8.3 Probability of superiority\n",
    "\n",
    "Another way to report the effect size is to report the probability of\n",
    "a point taken at random from one group has a larger value than another\n",
    "point taken at random from the other group.\n",
    "\n",
    "If we assume the data are normally distributed, we can compute the\n",
    "probability of superiority from Cohen's d using the following expression:\n",
    "\n",
    "$$\n",
    "ps = \\Phi ( \\frac {\\delta} {\\sqrt {2}} )\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\Phi$ is the cumulative Normal distribution\n",
    "- $\\delta$ is the Cohen's d\n",
    "\n",
    "If one agrees with the normality assumption, one can calculate the\n",
    "probability of superiority from Cohen's d. If not, one can compute\n",
    "the probability of superiority directly **from the posterior** by\n",
    "taking random samples from two groups and counting how many times\n",
    "a random value from one group is greater than a random value from\n",
    "the other group. This calculation is an example of an advantage of\n",
    "using **Markov Chain Monte Carlo (MCMC)** methods. That is, once we\n",
    "get samples from the posterior, we can compute **many quantities**\n",
    "often in ways that are easier than with other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7468d30dbc01b61",
   "metadata": {},
   "source": [
    "#### 2.8.4 Posterior analysis of mean differences\n",
    "\n",
    "To conclude our previous discussion, let's compute the posterior\n",
    "distribution for differences in means, Cohen's d, and the probability\n",
    "of superiority and combine them into a single plot."
   ]
  },
  {
   "cell_type": "code",
   "id": "e0144a2fea3f07a6",
   "metadata": {},
   "source": [
    "cg_posterior = az.extract(idata_cg)\n",
    "\n",
    "dist= pz.Normal(0, 1)\n",
    "\n",
    "comparisons = [(categories[i], categories[j]) for i in range(4) for j in range(i + 1, 4)]\n",
    "\n",
    "_, axes = plt.subplots(3, 2, figsize=(13, 9), sharex=True)\n",
    "for (i, j), ax in zip(comparisons, axes.ravel()):\n",
    "    means_diff = cg_posterior['\\u03bc'].sel(days=i) - cg_posterior['\\u03bc'].sel(days=j)\n",
    "\n",
    "    d_cohen = (means_diff / np.sqrt((cg_posterior['\\u03c3'].sel(days=i)**2 +\n",
    "                                     cg_posterior['\\u03c3'].sel(days=j)**2) / 2)).mean().item()\n",
    "\n",
    "    ps = dist.cdf(d_cohen / (2 ** 0.5))\n",
    "\n",
    "    az.plot_posterior(means_diff.values, ref_val=0, ax=ax)\n",
    "    ax.set_title(f'{i} - {j}')\n",
    "    ax.plot(0, label=f\"Cohen's d = {d_cohen:.2f}\\nProb sup = {ps:.2f}\", alpha=0)\n",
    "    ax.legend(loc=1)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "72f4e569e42b7387",
   "metadata": {},
   "source": [
    "One way to read this figure is to compare the reference value of zero\n",
    "difference with the HDI interval. We observe that the HDI interval\n",
    "**does not** contain the zero difference reference value in a\n",
    "single comparison: Thursday and Sunday. For all other pairs of days,\n",
    "we cannot, statistically, rule out a difference of zero.\n",
    "\n",
    "However, even comparing Sunday and Thursday, we observe that the average\n",
    "difference is only $0.48. Is that difference large enough to matter?\n",
    "\n",
    "The short answer: these kinds of questions can only be informed by\n",
    "statistics; they cannot be answered by statistics. Formally, we would\n",
    "need to define a loss function or at least the definition of some\n",
    "threshold value for the effect size which is informed by other values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ef071146cdad04",
   "metadata": {},
   "source": [
    "### 2.9 Summary\n",
    "\n",
    "Although Bayesian statistics is simple, fully probabilistic models\n",
    "often lead to analytically intractable expressions. Although this\n",
    "tendency has historically been a barrier to widespread adoption of\n",
    "Bayesian methods, recent mathematics, statistics, physics, and\n",
    "computer science allow us to solve - at least in principle -\n",
    "any inference problem.\n",
    "\n",
    "We introduced the PyMC library. To add a probability distribution\n",
    "to a model, we only need a single line of code. Using this library,\n",
    "distributions can be used as priors (unobserved variables) or\n",
    "likelihoods (observed variables).\n",
    "\n",
    "Sampling can be achieved with a single line as well. PyMC allows us\n",
    "to sample the posterior. If everything has gone right, these samples\n",
    "are representative of the correct posterior distribution. Further,\n",
    "these samples will be a representation of the logical consequences\n",
    "of our hypothesize model and our observed data.\n",
    "\n",
    "We can explore the posterior generated by PyMC using ArviZ to help\n",
    "us interpret and visualize posterior distributions. One particular\n",
    "way of using the posterior is to help us make inference decisions by\n",
    "comparing the ROPE (region of practical equivalence) against the\n",
    "HDI interval.\n",
    "\n",
    "Finally, although we only mention this usage in passing, we briefly\n",
    "mention loss functions. A loss function is a formal way to quantify\n",
    "the trade-offs and costs associate with making a decision in the\n",
    "presence of uncertainty.\n",
    "\n",
    "Although all our examples so far have used a (simple) one-parameter\n",
    "model, generalizing to an arbitrary number of parameters is trivial\n",
    "with PyMC. We exemplify how to do this with the Gaussian and Student's\n",
    "T models. Specifically, we showed how to use the Student's T\n",
    "distribution to perform robust inference in the presence of \"outliers.\"\n",
    "In the next chapters, we'll look at how these models can be used as\n",
    "part of linear regression models.\n",
    "\n",
    "Additionally, we used a Gaussian model to compare groups. Although this\n",
    "analysis is sometimes framed in the context of hypothesis testing, we\n",
    "frame this task as a problem of inferring the effect size."
   ]
  },
  {
   "cell_type": "code",
   "id": "2048a0a8321074ad",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
